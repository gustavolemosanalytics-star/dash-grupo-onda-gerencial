# Setup BigQuery + Cloud Run - Dashboard Grupo Onda

Guia completo para migrar de Google Sheets para BigQuery + Cloud Run

## üìä Arquitetura

```
Frontend (Vercel) ‚Üí Backend (Cloud Run) ‚Üí BigQuery (3 tabelas)
                         ‚Üì
                  Service Account + IAM
```

---

## Passo 1: Criar Tabelas no BigQuery

### 1.1 Acessar BigQuery Console

1. Acesse [BigQuery Console](https://console.cloud.google.com/bigquery)
2. Selecione projeto: `cortex-analytics-479819`
3. Confirme que o dataset `grupo_onda` existe

### 1.2 Executar SQL de Cria√ß√£o

No editor de queries do BigQuery, execute o arquivo [bigquery_setup.sql](bigquery_setup.sql):

```sql
-- Copie e cole TODO o conte√∫do do arquivo bigquery_setup.sql
-- Ele criar√° as 3 tabelas:
-- 1. planejamento
-- 2. bar_zig
-- 3. vendas_ingresso
```

‚úÖ **Resultado esperado**: 3 tabelas criadas + 2 materialized views

---

## Passo 2: Importar Dados Existentes

### Op√ß√£o A: Upload via Console (Recomendado para primeira vez)

1. **Para bar_zig:**
   ```
   - V√° em `gerenciamento_grupo_onda`
   - Clique em `bar_zig` ‚Üí "Load Data"
   - Source: "Upload" ‚Üí Selecione `backend/data/bar_zig.csv`
   - File format: CSV
   - Auto-detect schema: OFF (use schema definido)
   - Write preference: Append to table
   ```

2. **Para vendas_ingresso:**
   ```
   - Mesmo processo, usando `backend/data/vendas_ingresso.csv`
   ```

3. **Para planejamento:**
   ```
   - Exporte a planilha Google Sheets como CSV
   - Upload no BigQuery
   ```

### Op√ß√£o B: Importar direto do Google Sheets

```sql
-- Bar (da planilha Google Sheets)
LOAD DATA OVERWRITE `cortex-analytics-479819.grupo_onda.bar_zig`
FROM FILES (
  format = 'CSV',
  uris = ['https://docs.google.com/spreadsheets/d/1C68_TiIwrYjuFszxdJmiIUwq7Kk42MIMv2-ysOSFpR4/export?format=csv&gid=0']
);

-- Vendas (da planilha Google Sheets)
LOAD DATA OVERWRITE `cortex-analytics-479819.grupo_onda.vendas_ingresso`
FROM FILES (
  format = 'CSV',
  uris = ['https://docs.google.com/spreadsheets/d/1hHFo0sJnh4nAQbo0U2R1Kx95KTbyuImMu_C67YgELm0/export?format=csv&gid=0']
);

-- Planejamento
LOAD DATA OVERWRITE `cortex-analytics-479819.grupo_onda.planejamento`
FROM FILES (
  format = 'CSV',
  uris = ['https://docs.google.com/spreadsheets/d/1t1KVI9E6GanMnrNR55U0ssM46GBiWN_d5Ux9qVACDi8/export?format=csv']
);
```

---

## Passo 3: Criar Service Account

### 3.1 Criar Service Account

```bash
gcloud iam service-accounts create dashboard-backend \
  --display-name="Dashboard Backend Service Account" \
  --project=cortex-analytics-479819
```

### 3.2 Dar Permiss√µes

```bash
# BigQuery Data Viewer
gcloud projects add-iam-policy-binding cortex-analytics-479819 \
  --member="serviceAccount:dashboard-backend@cortex-analytics-479819.iam.gserviceaccount.com" \
  --role="roles/bigquery.dataViewer"

# BigQuery Job User (para rodar queries)
gcloud projects add-iam-policy-binding cortex-analytics-479819 \
  --member="serviceAccount:dashboard-backend@cortex-analytics-479819.iam.gserviceaccount.com" \
  --role="roles/bigquery.jobUser"
```

### 3.3 Criar Chave JSON

```bash
gcloud iam service-accounts keys create dashboard-sa-key.json \
  --iam-account=dashboard-backend@cortex-analytics-479819.iam.gserviceaccount.com
```

‚ö†Ô∏è **IMPORTANTE**: Guarde o arquivo `dashboard-sa-key.json` em local seguro!

---

## Passo 4: Configurar Backend

### 4.1 Instalar Depend√™ncias

```bash
cd backend
pip install google-cloud-bigquery
```

J√° est√° configurado em `requirements.txt`:
```
google-cloud-bigquery==3.14.1
```

### 4.2 Vari√°veis de Ambiente

Criar `.env` (local) ou configurar no Cloud Run:

```env
# BigQuery
GCP_PROJECT_ID=cortex-analytics-479819
BIGQUERY_DATASET=grupo_onda
GOOGLE_APPLICATION_CREDENTIALS=./dashboard-sa-key.json  # Local
# No Cloud Run, use service account attachment

# Server
PORT=8080  # Cloud Run usa 8080
DEBUG=false
```

---

## Passo 5: Deploy no Cloud Run

### 5.1 Criar Dockerfile

J√° inclu√≠do no projeto em `backend/Dockerfile`:

```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD exec uvicorn main:app --host 0.0.0.0 --port ${PORT:-8080}
```

### 5.2 Build e Deploy

```bash
# Autenticar
gcloud auth login
gcloud config set project cortex-analytics-479819

# Build
cd backend
gcloud builds submit --tag gcr.io/cortex-analytics-479819/dashboard-backend

# Deploy no Cloud Run
gcloud run deploy dashboard-backend \
  --image gcr.io/cortex-analytics-479819/dashboard-backend \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated \
  --service-account dashboard-backend@cortex-analytics-479819.iam.gserviceaccount.com \
  --set-env-vars "GCP_PROJECT_ID=cortex-analytics-479819,BIGQUERY_DATASET=grupo_onda" \
  --memory 512Mi \
  --cpu 1 \
  --min-instances 0 \
  --max-instances 10
```

### 5.3 Pegar URL do Cloud Run

Ap√≥s deploy, voc√™ receber√° uma URL tipo:
```
https://dashboard-backend-XXXXX-uc.a.run.app
```

---

## Passo 6: Atualizar Frontend no Vercel

1. No Vercel ‚Üí Settings ‚Üí Environment Variables
2. Atualizar:
   ```
   VITE_API_URL = https://dashboard-backend-XXXXX-uc.a.run.app
   ```
3. Redeploy

---

## üìä Custos Estimados (Free Tier)

### BigQuery
- **Queries**: 1 TB/m√™s gr√°tis
- **Storage**: 10 GB/m√™s gr√°tis
- Estimativa: **$0/m√™s** (dentro do free tier)

### Cloud Run
- **Invoca√ß√µes**: 2M/m√™s gr√°tis
- **CPU**: 180k vCPU-seconds/m√™s
- **Mem√≥ria**: 360k GiB-seconds/m√™s
- Estimativa: **$0-5/m√™s**

### Total: ~$0-5/m√™s üéâ

---

## üîÑ Atualiza√ß√£o de Dados

### Op√ß√£o 1: Scheduled Queries (Recomendado)

Configure queries agendadas no BigQuery para importar do Google Sheets automaticamente:

```sql
-- Agendar para rodar diariamente
CREATE OR REPLACE SCHEDULED QUERY sync_bar_from_sheets
OPTIONS (
  schedule = 'every day 03:00',
  time_zone = 'America/Sao_Paulo'
)
AS
LOAD DATA OVERWRITE `cortex-analytics-479819.grupo_onda.bar_zig`
FROM FILES (
  format = 'CSV',
  uris = ['URL_DO_GOOGLE_SHEETS']
);
```

### Op√ß√£o 2: Cloud Functions

Criar function que roda periodicamente (Cloud Scheduler):
- L√™ Google Sheets
- Insere no BigQuery
- Dispara notifica√ß√£o

### Op√ß√£o 3: Manual

```bash
bq load --replace \
  --source_format=CSV \
  --autodetect \
  gerenciamento_grupo_onda.bar_zig \
  gs://seu-bucket/bar_zig.csv
```

---

## üß™ Testar

### 1. Testar BigQuery

```sql
SELECT COUNT(*) FROM `cortex-analytics-479819.grupo_onda.bar_zig`;
SELECT COUNT(*) FROM `cortex-analytics-479819.grupo_onda.vendas_ingresso`;
SELECT COUNT(*) FROM `cortex-analytics-479819.grupo_onda.planejamento`;
```

### 2. Testar Backend

```bash
curl https://dashboard-backend-XXXXX-uc.a.run.app/api/health
```

Deve retornar:
```json
{
  "status": "ok",
  "bigquery": true
}
```

### 3. Testar Frontend

Abrir no navegador e ver dados carregando!

---

## üöÄ Vantagens dessa Arquitetura

‚úÖ **Performance**: Queries SQL nativas (100x mais r√°pido que CSV)
‚úÖ **Escalabilidade**: BigQuery escala para petabytes
‚úÖ **Custo**: Gr√°tis at√© 1TB de queries/m√™s
‚úÖ **Infraestrutura √önica**: Tudo no GCP
‚úÖ **Profissional**: Stack enterprise-grade
‚úÖ **Cache Autom√°tico**: BigQuery cacheia queries
‚úÖ **Seguran√ßa**: IAM roles granulares
‚úÖ **Monitoramento**: Cloud Logging + Monitoring inclusos

---

## üìù Pr√≥ximos Passos

1. ‚úÖ Executar `bigquery_setup.sql` no BigQuery Console
2. ‚úÖ Importar CSVs para as tabelas
3. ‚úÖ Criar service account com permiss√µes
4. ‚úÖ Deploy backend no Cloud Run
5. ‚úÖ Atualizar VITE_API_URL no Vercel
6. ‚úÖ Testar tudo funcionando
7. üîÑ Configurar sync autom√°tico (opcional)

---

**D√∫vidas?** Consulte:
- [BigQuery Docs](https://cloud.google.com/bigquery/docs)
- [Cloud Run Docs](https://cloud.google.com/run/docs)
